================================================================================                                       LOG================================================================================Current Queue (18/1/2020 -> Sync Up: 11/1/2020):------------------------------------------------0) Copy over content to template (Anish & Xiao Lei)1) Find citations for all claims (Anish & Xiao Lei)2) Verify our imitation method is Behaviour Cloning via Forward Learning (Anish)    -> What is the error function/reward function in existing systems3) Why is it more effiecient to use Behavioural Cloning vs Inverse Reinforcement (Anish)Learning?    -> This part of intro could be a little better    -> We learn the q-values of the expert        -> Its easier to create the q-values since you already have the reward function        -> Do we not learn the q-values by mimicing the experts reward function (inverse)    -> Its much easier to do reinforcement learning by having a reward function (behaviour    cloning) vs trying to model the ideal reward function (inverse reinforcement learning)4) Explain our Reinforcement Learning method (Anish)5) Give mathematical explanation of PID (Anish)7) Mathematical Framework (Anish & Xiao Lei)    -> Summarize mathematical abstraction from other papers (Anish)    -> Cite other papers (Anish)    -> Write RL algo briefly (Anish)8) Write implementation section (Lead: Xiao Lei, Anish)    -> CartPole Project    -> Site the repo    -> Implementation details    -> Mostly explaining the specifics of Cartpole and the relationship with our project9) Write results section (Xiao Lei)    -> Plots    -> Hyperparametrs Tuning/Network Calibration    -> Result IL250RL250        1) Results was better than PID model            -> PID500? (might not need 500 but would be good to do > 50) PID/IL50 (Done)        2) Results was better than pure RL model            -> IL250RL250 was better than RL250 alone            -> Make RL500 Model (Done)    -> Analyze results10) Conclusion (Anish & Xiao Lei)Previous Queue (Target 29/12/2019):----------------------------------1) Research similar work briefly (Anish) (90% done)    -> Look specifically into behavioural cloning and what is similar/different    -> It looks as if the RL portion at the end is novel    -> Inverse reinforcement learning looks interesting2) Summarize and write findings of (1)  (Anish) (80% done)    -> Rough introduction2.5) Write the problem formulation (Mathematical Abstraction) (both do seperately and compare)    -> One-shot imitation learning    -> Inverse reinforcement learning    -> Apprenticeship learning (not as good but relevant)3) Train RL250 alone (Xiao Lei) (done)4) Train another IL250_RL250 with new reward (Xiao Lei) (done)    -> Supports results!Better name needed!!5) Write solution (RL Augmented with Behavioural Cloning or another name) section -> One Shot Learning paper has a good explantion of their solution   (a) Exploration rate (Xiao Lei)   (b) Learning rate (Xiao Lei)   (c) Num epochs (Xiao Lei)6) Write neural network layer visualization (need to do)    (a) Get each activation weight (Xiao Lei) (Done)    (b) Aggregate weights into list  (Xiao Lei)  (Done)    (c) Make contour plots (Xiao Lei)  (Done)        -> https://stackoverflow.com/questions/34732305/contour-plot-of-2d-array-in-matplotlib7) Review findings and get isolated data for exploration rate, learning rate, num epochs (Both discuss plan, Xiao Lei runs)Final Queue:------------1) Get all graphs/visualization finalized2) Get network visualizations finalized2) Write findings sections3) Write conclusion4) Write abstract5) Review paper6) Complete and submit!!!